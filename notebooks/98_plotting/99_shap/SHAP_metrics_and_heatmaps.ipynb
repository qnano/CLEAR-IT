{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e0c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from clearit.config import OUTPUTS_DIR\n",
    "from clearit.shap.io import load_shap_bundle\n",
    "from clearit.shap.compute import smooth_shap_maps\n",
    "from clearit.plotting.shap import plot_shap_heatmaps\n",
    "from clearit.shap.metrics import (\n",
    "    importance_matrix_abs_mean,\n",
    "    infer_pairs_by_name,\n",
    "    diagonality_index,\n",
    "    channel_specificity_entropy,\n",
    "    center_surround_metrics,\n",
    "    to_dataframe_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096e910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SHAP_FN_highest_row.png and metrics for SHAP_FN_highest\n",
      "Saved SHAP_FN_lowest_row.png and metrics for SHAP_FN_lowest\n",
      "Saved SHAP_FP_highest_row.png and metrics for SHAP_FP_highest\n",
      "Saved SHAP_FP_lowest_row.png and metrics for SHAP_FP_lowest\n",
      "Saved SHAP_TN_highest_row.png and metrics for SHAP_TN_highest\n",
      "Saved SHAP_TN_lowest_row.png and metrics for SHAP_TN_lowest\n",
      "Saved SHAP_TP_highest_row.png and metrics for SHAP_TP_highest\n",
      "Saved SHAP_TP_lowest_row.png and metrics for SHAP_TP_lowest\n"
     ]
    }
   ],
   "source": [
    "test_id = \"T0030\"\n",
    "num_per_marker = 100\n",
    "dataset_name = \"TNBC1-MxIF8\"\n",
    "annotation_name = \"TME-A_ML6\"\n",
    "input_dir = Path(OUTPUTS_DIR, \"shap\", dataset_name, annotation_name, test_id, f\"N{num_per_marker:04d}\")\n",
    "output_dir = Path(\"shap_aggregate\", dataset_name, annotation_name, test_id, f\"N{num_per_marker:04d}\")\n",
    "\n",
    "# Plotting knobs\n",
    "NORMALIZE = \"row\"          # \"none\" | \"global\" | \"row\" | \"column\"\n",
    "APPLY_SMOOTHING = True\n",
    "SIGMA = 2.0\n",
    "OVERLAY_FMT = \"%.2g\"       # formatting of numbers in tiles\n",
    "OVERLAY_LOC = \"lower-right\"\n",
    "SAVE = True\n",
    "\n",
    "yaml_files = sorted(input_dir.glob(\"*.yaml\"))\n",
    "if not yaml_files:\n",
    "    print(f\"No bundles found in {input_dir}\")\n",
    "else:\n",
    "    for yml in yaml_files:\n",
    "        base = yml.with_suffix(\"\")  # stem without extension (e.g., SHAP_TP_highest)\n",
    "        shap_values, df_cells, meta = load_shap_bundle(base)\n",
    "\n",
    "        labels = meta.get(\"labels\", {})\n",
    "        chan_names  = labels.get(\"channel_strings\")\n",
    "        class_names = labels.get(\"class_strings\")\n",
    "        order       = labels.get(\"desired_channel_order\")\n",
    "\n",
    "        # Reorder channels if specified in metadata (keeps names in sync)\n",
    "        if order:\n",
    "            shap_values = shap_values[:, order, :, :, :]\n",
    "            if chan_names:\n",
    "                chan_names = [chan_names[i] for i in order]\n",
    "\n",
    "        # Optional smoothing at load time\n",
    "        if APPLY_SMOOTHING and not meta.get(\"shap\", {}).get(\"smoothed\", False):\n",
    "            shap_values = smooth_shap_maps(shap_values, sigma=SIGMA)\n",
    "\n",
    "        # ---------- compute metrics ----------\n",
    "        I = importance_matrix_abs_mean(shap_values)              # (C,K)\n",
    "        if NORMALIZE == \"row\":\n",
    "            I = I / I.sum(axis=0, keepdims=True)\n",
    "        elif NORMALIZE == \"column\":\n",
    "            I = I / I.sum(axis=1, keepdims=True)\n",
    "        elif NORMALIZE == \"global\":\n",
    "            I = I / I.max()\n",
    "\n",
    "        pairs = infer_pairs_by_name(chan_names or [], class_names or [])\n",
    "        diag_val = diagonality_index(I, pairs)\n",
    "        ent = channel_specificity_entropy(I)                     # (K,)\n",
    "        cs  = center_surround_metrics(shap_values)               # dict of (C,K)\n",
    "\n",
    "        # Decide save dir for this bundle\n",
    "        save_dir = (output_dir or base.parent)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Write metrics CSVs\n",
    "        stem = base.name\n",
    "        to_dataframe_matrix(I, chan_names, class_names).to_csv(save_dir / f\"{stem}_importance_absmean.csv\", float_format=\"%.6g\")\n",
    "        pd.DataFrame({\"diagonality_index\": [diag_val], \"num_pairs\": [len(pairs)]}).to_csv(save_dir / f\"{stem}_diagonality.csv\", index=False)\n",
    "        pd.DataFrame({\n",
    "            \"class\": class_names or [f\"Class {k}\" for k in range(I.shape[1])],\n",
    "            \"entropy_nats\": ent,\n",
    "            \"entropy_bits\": ent / np.log(2)\n",
    "        }).to_csv(save_dir / f\"{stem}_class_entropy.csv\", index=False, float_format=\"%.6g\")\n",
    "\n",
    "        for key, M in cs.items():  # center_abs_share, CSI, sign_inversion_frac\n",
    "            to_dataframe_matrix(M, chan_names, class_names).to_csv(save_dir / f\"{stem}_{key}.csv\", float_format=\"%.6g\")\n",
    "\n",
    "        # ---------- plot with overlay (importance) ----------\n",
    "        fig, _ = plot_shap_heatmaps(\n",
    "            shap_values,\n",
    "            channel_strings=chan_names,\n",
    "            class_strings=class_names,\n",
    "            title=None,\n",
    "            average_over_batch=True,\n",
    "            figsize_multiplier=2.0,\n",
    "            normalize=NORMALIZE,\n",
    "            overlay_matrix=I,\n",
    "            overlay_fmt=OVERLAY_FMT,\n",
    "            overlay_loc=OVERLAY_LOC,\n",
    "            font_scale=1.5,\n",
    "            cbar_width_scale=1.5,\n",
    "        )\n",
    "\n",
    "        out_png = save_dir / f\"{stem}_{NORMALIZE}.png\"\n",
    "        if SAVE:\n",
    "            fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "            print(f\"Saved {out_png.name} and metrics for {stem}\")\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02557f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SHAP_FN_highest_row.png and metrics for SHAP_FN_highest\n",
      "Saved SHAP_FN_lowest_row.png and metrics for SHAP_FN_lowest\n",
      "Saved SHAP_FP_highest_row.png and metrics for SHAP_FP_highest\n",
      "Saved SHAP_FP_lowest_row.png and metrics for SHAP_FP_lowest\n",
      "Saved SHAP_TN_highest_row.png and metrics for SHAP_TN_highest\n",
      "Saved SHAP_TN_lowest_row.png and metrics for SHAP_TN_lowest\n",
      "Saved SHAP_TP_highest_row.png and metrics for SHAP_TP_highest\n",
      "Saved SHAP_TP_lowest_row.png and metrics for SHAP_TP_lowest\n"
     ]
    }
   ],
   "source": [
    "test_id = \"T0129\"\n",
    "num_per_marker = 100\n",
    "dataset_name = \"TNBC2-MIBI/TNBC2-MIBI8\"\n",
    "annotation_name = \"TME-A_ML6\"\n",
    "input_dir = Path(OUTPUTS_DIR, \"shap\", dataset_name, annotation_name, test_id, f\"N{num_per_marker:04d}\")\n",
    "output_dir = Path(\"shap_aggregate\", dataset_name, annotation_name, test_id, f\"N{num_per_marker:04d}\")\n",
    "\n",
    "# Plotting knobs\n",
    "NORMALIZE = \"row\"          # \"none\" | \"global\" | \"row\" | \"column\"\n",
    "APPLY_SMOOTHING = True\n",
    "SIGMA = 2.0\n",
    "OVERLAY_FMT = \"%.2g\"       # formatting of numbers in tiles\n",
    "OVERLAY_LOC = \"lower-right\"\n",
    "SAVE = True\n",
    "\n",
    "yaml_files = sorted(input_dir.glob(\"*.yaml\"))\n",
    "if not yaml_files:\n",
    "    print(f\"No bundles found in {input_dir}\")\n",
    "else:\n",
    "    for yml in yaml_files:\n",
    "        base = yml.with_suffix(\"\")  # stem without extension (e.g., SHAP_TP_highest)\n",
    "        shap_values, df_cells, meta = load_shap_bundle(base)\n",
    "\n",
    "        labels = meta.get(\"labels\", {})\n",
    "        chan_names  = labels.get(\"channel_strings\")\n",
    "        class_names = labels.get(\"class_strings\")\n",
    "        order       = labels.get(\"desired_channel_order\")\n",
    "\n",
    "        # Reorder channels if specified in metadata (keeps names in sync)\n",
    "        if order:\n",
    "            shap_values = shap_values[:, order, :, :, :]\n",
    "            if chan_names:\n",
    "                chan_names = [chan_names[i] for i in order]\n",
    "\n",
    "        # Optional smoothing at load time\n",
    "        if APPLY_SMOOTHING and not meta.get(\"shap\", {}).get(\"smoothed\", False):\n",
    "            shap_values = smooth_shap_maps(shap_values, sigma=SIGMA)\n",
    "\n",
    "        # ---------- compute metrics ----------\n",
    "        I = importance_matrix_abs_mean(shap_values)              # (C,K)\n",
    "        if NORMALIZE == \"row\":\n",
    "            I = I / I.sum(axis=0, keepdims=True)\n",
    "        elif NORMALIZE == \"column\":\n",
    "            I = I / I.sum(axis=1, keepdims=True)\n",
    "        elif NORMALIZE == \"global\":\n",
    "            I = I / I.max()\n",
    "\n",
    "        pairs = infer_pairs_by_name(chan_names or [], class_names or [])\n",
    "        diag_val = diagonality_index(I, pairs)\n",
    "        ent = channel_specificity_entropy(I)                     # (K,)\n",
    "        cs  = center_surround_metrics(shap_values)               # dict of (C,K)\n",
    "\n",
    "        # Decide save dir for this bundle\n",
    "        save_dir = (output_dir or base.parent)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Write metrics CSVs\n",
    "        stem = base.name\n",
    "        to_dataframe_matrix(I, chan_names, class_names).to_csv(save_dir / f\"{stem}_importance_absmean.csv\", float_format=\"%.6g\")\n",
    "        pd.DataFrame({\"diagonality_index\": [diag_val], \"num_pairs\": [len(pairs)]}).to_csv(save_dir / f\"{stem}_diagonality.csv\", index=False)\n",
    "        pd.DataFrame({\n",
    "            \"class\": class_names or [f\"Class {k}\" for k in range(I.shape[1])],\n",
    "            \"entropy_nats\": ent,\n",
    "            \"entropy_bits\": ent / np.log(2)\n",
    "        }).to_csv(save_dir / f\"{stem}_class_entropy.csv\", index=False, float_format=\"%.6g\")\n",
    "\n",
    "        for key, M in cs.items():  # center_abs_share, CSI, sign_inversion_frac\n",
    "            to_dataframe_matrix(M, chan_names, class_names).to_csv(save_dir / f\"{stem}_{key}.csv\", float_format=\"%.6g\")\n",
    "\n",
    "        # ---------- plot with overlay (importance) ----------\n",
    "        fig, _ = plot_shap_heatmaps(\n",
    "            shap_values,\n",
    "            channel_strings=chan_names,\n",
    "            class_strings=class_names,\n",
    "            title=None,\n",
    "            average_over_batch=True,\n",
    "            figsize_multiplier=2.0,\n",
    "            normalize=NORMALIZE,\n",
    "            overlay_matrix=I,\n",
    "            overlay_fmt=OVERLAY_FMT,\n",
    "            overlay_loc=OVERLAY_LOC,\n",
    "            font_scale=1.5,\n",
    "            cbar_width_scale=1.5,\n",
    "        )\n",
    "\n",
    "        out_png = save_dir / f\"{stem}_{NORMALIZE}.png\"\n",
    "        if SAVE:\n",
    "            fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "            print(f\"Saved {out_png.name} and metrics for {stem}\")\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
